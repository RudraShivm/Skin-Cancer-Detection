{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca1b367",
   "metadata": {},
   "source": [
    "# ISIC 2024 — Skin Cancer Detection (Inference Only)\n",
    "\n",
    "**This notebook is for Kaggle submission only.** It loads pre-trained checkpoints\n",
    "from attached datasets and generates `submission.csv`.\n",
    "\n",
    "### Required Data Sources\n",
    "1. **Competition data**: `isic-2024-challenge` (provides `test-image.hdf5`, `sample_submission.csv`)\n",
    "2. **Checkpoint datasets**: Your training notebook outputs (e.g., `efficientnet-b0`)\n",
    "\n",
    "### How checkpoint paths work\n",
    "When you add a Kaggle notebook output as a dataset named e.g. `efficientnet-b0`,\n",
    "it appears at: `/kaggle/input/efficientnet-b0/checkpoints/efficientnet_b0/fold_0/...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af632bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION — Edit these for your setup\n",
    "# ============================================================\n",
    "import os, sys, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Map of model_name -> Kaggle dataset slug where its checkpoints live.\n",
    "# When you add a notebook output as a dataset, Kaggle mounts it at:\n",
    "#   /kaggle/input/{dataset-slug}/\n",
    "#\n",
    "# Example: If your training notebook output is named \"efficientnet-b0\",\n",
    "# and checkpoints are at /kaggle/input/efficientnet-b0/checkpoints/efficientnet_b0/fold_0/\n",
    "MODEL_DATASETS = {\n",
    "    \"efficientnet_b0\": \"rudrashivm/efficientnet-b0\",\n",
    "    # \"convnext_tiny\":   \"convnext-tiny\",\n",
    "    # \"swin_large\":      \"swin-large\",\n",
    "}\n",
    "\n",
    "# Ensemble strategy: \"soft\" or \"hard_weighted\"\n",
    "ENSEMBLE_STRATEGY = \"soft\"\n",
    "\n",
    "# Inference batch size (increase if GPU memory allows)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Competition data\n",
    "DATA_DIR = \"/kaggle/input/isic-2024-challenge\"\n",
    "TEST_HDF5 = os.path.join(DATA_DIR, \"test-image.hdf5\")\n",
    "SAMPLE_SUB = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# Verify data sources\n",
    "print(\"Data sources:\")\n",
    "for path, label in [(TEST_HDF5, \"test HDF5\"), (SAMPLE_SUB, \"sample submission\")]:\n",
    "    status = \"✅\" if os.path.exists(path) else \"❌\"\n",
    "    print(f\"  {status} {label}: {path}\")\n",
    "\n",
    "print(\"\\nCheckpoint datasets:\")\n",
    "for model_name, dataset_slug in MODEL_DATASETS.items():\n",
    "    base = f\"/kaggle/input/{dataset_slug}\"\n",
    "    ckpt_dir = os.path.join(base, \"checkpoints\", model_name)\n",
    "    status = \"✅\" if os.path.exists(ckpt_dir) else \"❌\"\n",
    "    print(f\"  {status} {model_name}: {ckpt_dir}\")\n",
    "    if os.path.exists(ckpt_dir):\n",
    "        for fold_dir in sorted(os.listdir(ckpt_dir)):\n",
    "            fold_path = os.path.join(ckpt_dir, fold_dir)\n",
    "            if os.path.isdir(fold_path):\n",
    "                ckpts = [f for f in os.listdir(fold_path) if f.endswith('.ckpt')]\n",
    "                print(f\"      {fold_dir}: {ckpts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc32782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL & TRANSFORMS (self-contained — no repo clone needed)\n",
    "# ============================================================\n",
    "# This cell defines everything needed to load checkpoints and\n",
    "# run inference. It mirrors src/models/isic_module.py and\n",
    "# src/data/components/transforms.py but is fully inline so the\n",
    "# notebook works in Kaggle's no-internet submission environment.\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from lightning import LightningModule\n",
    "from torchmetrics import MaxMetric, MeanMetric\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryROC\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "def get_val_transforms(img_size: int = 224):\n",
    "    \\\"\\\"\\\"Validation/test transforms: resize + normalize.\\\"\\\"\\\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "class ISICLitModule(LightningModule):\n",
    "    \\\"\\\"\\\"Minimal ISICLitModule for inference — matches training checkpoint structure.\\\"\\\"\\\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"\",\n",
    "        backbone: str = \"tf_efficientnet_b0_ns\",\n",
    "        num_classes: int = 1,\n",
    "        pretrained: bool = False,  # No need for pretrained weights at inference\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-2,\n",
    "        max_epochs: int = 20,\n",
    "        dropout: float = 0.0,\n",
    "        pos_weight: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            backbone,\n",
    "            pretrained=False,  # Weights come from checkpoint\n",
    "            num_classes=num_classes,\n",
    "            drop_rate=dropout,\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.tensor([pos_weight])\n",
    "        )\n",
    "\n",
    "        self.train_auroc = BinaryAUROC()\n",
    "        self.val_auroc = BinaryAUROC()\n",
    "        self.test_auroc = BinaryAUROC()\n",
    "        self.val_roc = BinaryROC()\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "        self.val_auroc_best = MaxMetric()\n",
    "\n",
    "        self.register_buffer(\"best_threshold\", torch.tensor(0.5))\n",
    "        self.register_buffer(\"best_auroc\", torch.tensor(0.0))\n",
    "\n",
    "        self._val_preds: List[torch.Tensor] = []\n",
    "        self._val_targets: List[torch.Tensor] = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "def get_model_img_size(model):\n",
    "    \\\"\\\"\\\"Auto-detect image size from TIMM backbone config.\\\"\\\"\\\"\n",
    "    try:\n",
    "        data_config = timm.data.resolve_data_config(model.model.pretrained_cfg)\n",
    "        return data_config.get(\"input_size\", (3, 224, 224))[-1]\n",
    "    except Exception:\n",
    "        return 224\n",
    "\n",
    "\n",
    "print(\"✅ Model and transforms defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE — Load checkpoints, predict on test-image.hdf5\n",
    "# ============================================================\n",
    "\n",
    "import h5py, cv2, glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load test ISIC IDs from sample submission\n",
    "test_df = pd.read_csv(SAMPLE_SUB)\n",
    "isic_ids = test_df[\"isic_id\"].tolist()\n",
    "print(f\"Test samples: {len(isic_ids)}\")\n",
    "\n",
    "# ---- Discover and load all model checkpoints ----\n",
    "model_configs = []  # List of (model, img_size, threshold, auroc)\n",
    "\n",
    "for model_name, dataset_slug in MODEL_DATASETS.items():\n",
    "    ckpt_base = f\"/kaggle/input/{dataset_slug}/checkpoints/{model_name}\"\n",
    "    \n",
    "    if not os.path.isdir(ckpt_base):\n",
    "        print(f\"⚠️  Skipping {model_name}: {ckpt_base} not found\")\n",
    "        continue\n",
    "    \n",
    "    # Discover fold directories\n",
    "    fold_dirs = sorted([\n",
    "        d for d in os.listdir(ckpt_base)\n",
    "        if d.startswith(\"fold_\") and os.path.isdir(os.path.join(ckpt_base, d))\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\n{model_name}: {len(fold_dirs)} folds\")\n",
    "    \n",
    "    for fold_dir in fold_dirs:\n",
    "        fold_path = os.path.join(ckpt_base, fold_dir)\n",
    "        \n",
    "        # Find best checkpoint (prefer auroc-named, fallback to last.ckpt)\n",
    "        auroc_ckpts = sorted(glob.glob(os.path.join(fold_path, \"epoch_*_auroc_*.ckpt\")))\n",
    "        if auroc_ckpts:\n",
    "            # Pick highest AUROC from filename\n",
    "            def get_auroc(p):\n",
    "                try: return float(os.path.basename(p).split(\"auroc_\")[-1].replace(\".ckpt\",\"\"))\n",
    "                except: return 0.0\n",
    "            ckpt_path = max(auroc_ckpts, key=get_auroc)\n",
    "        elif os.path.exists(os.path.join(fold_path, \"last.ckpt\")):\n",
    "            ckpt_path = os.path.join(fold_path, \"last.ckpt\")\n",
    "        else:\n",
    "            any_ckpts = glob.glob(os.path.join(fold_path, \"*.ckpt\"))\n",
    "            ckpt_path = any_ckpts[0] if any_ckpts else None\n",
    "        \n",
    "        if ckpt_path is None:\n",
    "            print(f\"  {fold_dir}: no checkpoint found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  {fold_dir}: {os.path.basename(ckpt_path)}\")\n",
    "        \n",
    "        model = ISICLitModule.load_from_checkpoint(ckpt_path, map_location=device)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        img_size = get_model_img_size(model)\n",
    "        threshold = model.best_threshold.item()\n",
    "        auroc = model.best_auroc.item() if hasattr(model, \"best_auroc\") else 0.5\n",
    "        print(f\"    img_size={img_size}, threshold={threshold:.4f}, auroc={auroc:.4f}\")\n",
    "        \n",
    "        model_configs.append((model, img_size, threshold, auroc))\n",
    "\n",
    "print(f\"\\nTotal model-folds loaded: {len(model_configs)}\")\n",
    "assert len(model_configs) > 0, \"No checkpoints loaded! Check MODEL_DATASETS paths.\"\n",
    "\n",
    "# ---- Run inference ----\n",
    "hdf5 = h5py.File(TEST_HDF5, \"r\")\n",
    "\n",
    "all_probs = []\n",
    "all_thresholds = []\n",
    "all_aurocs = []\n",
    "\n",
    "for model_idx, (model, img_size, threshold, auroc) in enumerate(model_configs):\n",
    "    print(f\"\\nModel {model_idx+1}/{len(model_configs)} (img_size={img_size})...\")\n",
    "    transform = get_val_transforms(img_size)\n",
    "    probs = []\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(isic_ids), BATCH_SIZE), desc=\"Batches\"):\n",
    "        batch_ids = isic_ids[batch_start : batch_start + BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        \n",
    "        for isic_id in batch_ids:\n",
    "            img_bytes = hdf5[isic_id][()]\n",
    "            img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = transform(image=img)[\"image\"]\n",
    "            batch_images.append(img)\n",
    "        \n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_tensor).squeeze(1)\n",
    "            batch_probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        probs.extend(batch_probs.tolist())\n",
    "    \n",
    "    all_probs.append(np.array(probs))\n",
    "    all_thresholds.append(threshold)\n",
    "    all_aurocs.append(auroc)\n",
    "    \n",
    "    # Free GPU memory\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "hdf5.close()\n",
    "\n",
    "# ---- Ensemble ----\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "if ENSEMBLE_STRATEGY == \"hard_weighted\":\n",
    "    weights = np.array(all_aurocs)\n",
    "    weights = weights / weights.sum() if weights.sum() > 0 else np.ones_like(weights) / len(weights)\n",
    "    hard = np.array([(p >= t).astype(float) for p, t in zip(all_probs, all_thresholds)])\n",
    "    final_probs = np.average(hard, axis=0, weights=weights)\n",
    "    final_threshold = 0.5\n",
    "    print(f\"\\nHard-weighted ensemble | weights: {[f'{w:.3f}' for w in weights]}\")\n",
    "else:\n",
    "    final_probs = np.mean(all_probs, axis=0)\n",
    "    final_threshold = np.mean(all_thresholds)\n",
    "    print(f\"\\nSoft ensemble | threshold: {final_threshold:.4f}\")\n",
    "\n",
    "final_preds = (final_probs >= final_threshold).astype(int)\n",
    "\n",
    "# ---- Generate submission.csv ----\n",
    "submission = pd.DataFrame({\n",
    "    \"isic_id\": isic_ids,\n",
    "    \"target\": final_preds,\n",
    "})\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, \"submission.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUBMISSION GENERATED: {output_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Samples: {len(submission)}\")\n",
    "print(f\"  Malignant: {final_preds.sum()} ({final_preds.mean()*100:.2f}%)\")\n",
    "print(f\"  Benign: {len(final_preds) - final_preds.sum()}\")\n",
    "print(f\"  Models×folds: {len(model_configs)}\")\n",
    "print(f\"  Strategy: {ENSEMBLE_STRATEGY}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission.head(10).to_string(index=False))\n",
    "print(f\"\\n✅ Ready for submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
