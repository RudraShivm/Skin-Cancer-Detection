{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4521c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: local\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def detect_platform():\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        return 'kaggle'\n",
    "    elif 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "PLATFORM = detect_platform()\n",
    "print(f\"Running on: {PLATFORM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de978d8-7eec-4a63-8699-8e6e7be37e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo directory: /home/rudrashivm/code/dl/Skin-Cancer-Detection\n",
      "Installing dependency packages...\n",
      "Installing the current project as an editable Python package...\n",
      "‚úÖ Setup complete\n"
     ]
    }
   ],
   "source": [
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle: repo must be attached as a dataset or cloned\n",
    "\n",
    "    os.system('git clone https://github.com/RudraShivm/Skin-Cancer-Detection.git /kaggle/working/skin-cancer-detection')\n",
    "    REPO_DIR = '/kaggle/working/skin-cancer-detection'\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Colab: clone from GitHub\n",
    "    # For private repo, use Colab secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        token = userdata.get('GITHUB_TOKEN')\n",
    "        clone_url = f'https://{token}@github.com/RudraShivm/Skin-Cancer-Detection.git'\n",
    "    except Exception:\n",
    "        clone_url = 'https://github.com/RudraShivm/Skin-Cancer-Detection.git'  # public\n",
    "    \n",
    "    os.system(f'git clone {clone_url} /content/skin-cancer-detection')\n",
    "    REPO_DIR = '/content/skin-cancer-detection'\n",
    "\n",
    "else:  # local\n",
    "    import rootutils\n",
    "    REPO_DIR = str(rootutils.find_root(search_from='.', indicator='.project-root'))\n",
    "\n",
    "print(f\"Repo directory: {REPO_DIR}\")\n",
    "\n",
    "# Change to repo directory and install\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Installing dependency packages...\")\n",
    "os.system('pip install -r requirements.txt -q')  # ‚Üê install packages\n",
    "print(f\"Installing the current project as an editable Python package...\")\n",
    "os.system('pip install -e . -q')                 # ‚Üê make src/ importable\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8016be7d-149b-483b-a215-b6d0392c537e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "WANDB_API_KEY invalid: API key may only contain the letters A-Z, digits and underscores.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m                     os.environ[key.strip()] = val.strip()\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33menv::PROJECT_ROOT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ.get(\u001b[33m'\u001b[39m\u001b[33mPROJECT_ROOT\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:101\u001b[39m, in \u001b[36mlogin\u001b[39m\u001b[34m(key, relogin, host, force, timeout, verify, referrer, anonymous)\u001b[39m\n\u001b[32m     94\u001b[39m     host = host.rstrip(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m _update_system_settings(\n\u001b[32m     97\u001b[39m     global_settings.read_system_settings(),\n\u001b[32m     98\u001b[39m     host=host,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m logged_in, _ = \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logged_in\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:190\u001b[39m, in \u001b[36m_login\u001b[39m\u001b[34m(key, relogin, host, force, timeout, verify, referrer, update_api_key, _silent)\u001b[39m\n\u001b[32m    182\u001b[39m     auth: wbauth.Auth | \u001b[38;5;28;01mNone\u001b[39;00m = _use_explicit_key(\n\u001b[32m    183\u001b[39m         key,\n\u001b[32m    184\u001b[39m         host=host_url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m         silent=_silent,\n\u001b[32m    188\u001b[39m     )\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     auth = \u001b[43m_find_or_prompt_for_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(auth, wbauth.AuthApiKey):\n\u001b[32m    200\u001b[39m     _verify_login(key=auth.api_key, base_url=auth.host.url)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:266\u001b[39m, in \u001b[36m_find_or_prompt_for_key\u001b[39m\u001b[34m(settings, host, force, relogin, referrer, input_timeout)\u001b[39m\n\u001b[32m    263\u001b[39m auth: wbauth.Auth | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     auth = \u001b[43mwbauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauthenticate_session\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwandb.login()\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_offline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_create\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m    277\u001b[39m     timed_out = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/lib/wbauth/authenticate.py:127\u001b[39m, in \u001b[36mauthenticate_session\u001b[39m\u001b[34m(host, source, no_offline, no_create, input_timeout, referrer, relogin)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relogin \u001b[38;5;129;01mand\u001b[39;00m (auth := session_credentials(host=host)):\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m auth\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relogin \u001b[38;5;129;01mand\u001b[39;00m (auth := \u001b[43m_use_system_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m auth\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/lib/wbauth/authenticate.py:188\u001b[39m, in \u001b[36m_use_system_auth\u001b[39m\u001b[34m(host, source)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_use_system_auth\u001b[39m(*, host: HostUrl, source: \u001b[38;5;28mstr\u001b[39m) -> Auth | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load (or reload) session credentials from external sources.\u001b[39;00m\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m \u001b[33;03m    Loads credentials from environment variables or the .netrc file.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m \u001b[33;03m        The new credentials, if any.\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    187\u001b[39m     auth = (\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m         \u001b[43m_try_env_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    189\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m wbnetrc.read_netrc_auth_with_source(host=host)\n\u001b[32m    190\u001b[39m     )\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _session_auth_lock:\n\u001b[32m    193\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m auth:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/lib/wbauth/authenticate.py:225\u001b[39m, in \u001b[36m_try_env_auth\u001b[39m\u001b[34m(host)\u001b[39m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m AuthWithSource(\n\u001b[32m    221\u001b[39m             auth=AuthApiKey(host=host, api_key=api_key),\n\u001b[32m    222\u001b[39m             source=env.API_KEY,\n\u001b[32m    223\u001b[39m         )\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m AuthenticationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv.API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m invalid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m identity_token_file:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AuthWithSource(\n\u001b[32m    229\u001b[39m         auth=AuthIdentityTokenFile(\n\u001b[32m    230\u001b[39m             host=host,\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m         source=env.IDENTITY_TOKEN_FILE,\n\u001b[32m    235\u001b[39m     )\n",
      "\u001b[31mAuthenticationError\u001b[39m: WANDB_API_KEY invalid: API key may only contain the letters A-Z, digits and underscores."
     ]
    }
   ],
   "source": [
    "# PROJECT_ROOT is needed by rootutils in train.py\n",
    "os.environ['PROJECT_ROOT'] = REPO_DIR\n",
    "\n",
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle secrets (set in Kaggle ‚Üí Add-ons ‚Üí Secrets)\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        os.environ['WANDB_API_KEY'] = secrets.get_secret('WANDB_API_KEY')\n",
    "    except Exception:\n",
    "        pass  # Secrets not available, skip\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Colab secrets (set in Colab ‚Üí üîë sidebar)\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "else:  # local\n",
    "    # Local: load from .env file if it exists\n",
    "    env_file = os.path.join(REPO_DIR, '.env')\n",
    "    if os.path.exists(env_file):\n",
    "        with open(env_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '=' in line:\n",
    "                    key, val = line.split('=', 1)\n",
    "                    os.environ[key.strip()] = val.strip()\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "print(f\"env::PROJECT_ROOT: {os.environ.get('PROJECT_ROOT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6311327a-b014-439e-b705-0ba6c4393082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:        /home/rudrashivm/code/dl/Skin-Cancer-Detection/data/isic-2024-challenge\n",
      "Checkpoints: /home/rudrashivm/code/dl/Skin-Cancer-Detection/checkpoints\n",
      "Outputs:     /home/rudrashivm/code/dl/Skin-Cancer-Detection/outputs\n",
      "‚úÖ Data found: ['sample_submission.csv', 'test-image.hdf5', 'test-metadata.csv', 'train-image.hdf5', 'train-image']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = None\n",
    "CHECKPOINT_DIR = None\n",
    "OUTPUT_DIR = None\n",
    "\n",
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle mounts competition data at /kaggle/input/\n",
    "    DATA_DIR = '/kaggle/input/competitions/isic-2024-challenge'\n",
    "    \n",
    "    # Outputs go to /kaggle/working/ (this is what gets saved)\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    OUTPUT_DIR = '/kaggle/working/outputs'\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Option A: Data in Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    DATA_DIR = '/content/drive/MyDrive/isic-2024-challenge'\n",
    "    \n",
    "    # Option B: Data downloaded to Colab's ephemeral disk\n",
    "    # DATA_DIR = '/content/isic-2024-challenge'\n",
    "    \n",
    "    CHECKPOINT_DIR = f'{REPO_DIR}/checkpoints'\n",
    "    OUTPUT_DIR = f'{REPO_DIR}/outputs'\n",
    "\n",
    "else:  # local\n",
    "    DATA_DIR = os.path.join(REPO_DIR, 'data', 'isic-2024-challenge')\n",
    "    CHECKPOINT_DIR = os.path.join(REPO_DIR, 'checkpoints')\n",
    "    OUTPUT_DIR = os.path.join(REPO_DIR, 'outputs')\n",
    "\n",
    "# Create dirs if needed\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data:        {DATA_DIR}\")\n",
    "print(f\"Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"Outputs:     {OUTPUT_DIR}\")\n",
    "\n",
    "# Verify data exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"‚úÖ Data found: {os.listdir(DATA_DIR)[:5]}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Data NOT found at {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdadacf-9365-44ef-9c24-34baf17fa475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:\n",
      "  python src/train.py data.data_dir=/home/rudrashivm/code/dl/Skin-Cancer-Detection/data/isic-2024-challenge experiment=isic_efficientnet_b0 trainer.accelerator=gpu extras.print_config=false extras.enforce_tags=false logger=wandb model.lr=0.00001 debug=default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2026-02-18 09:18:21,060\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - [rank: 0] Instantiating datamodule <src.data.isic_datamodule.ISICDataModule>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:21,675\u001b[0m][\u001b[34mh5py._conv\u001b[0m][\u001b[35mDEBUG\u001b[0m] - Creating converter from 7 to 5\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:21,676\u001b[0m][\u001b[34mh5py._conv\u001b[0m][\u001b[35mDEBUG\u001b[0m] - Creating converter from 5 to 7\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:21,676\u001b[0m][\u001b[34mh5py._conv\u001b[0m][\u001b[35mDEBUG\u001b[0m] - Creating converter from 7 to 5\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:21,676\u001b[0m][\u001b[34mh5py._conv\u001b[0m][\u001b[35mDEBUG\u001b[0m] - Creating converter from 5 to 7\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:23,498\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - [rank: 0] Instantiating model <src.models.isic_module.ISICLitModule>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,346\u001b[0m][\u001b[34mtimm.models._builder\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b0.ns_jft_in1k)\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,370\u001b[0m][\u001b[34mhttpcore.connection\u001b[0m][\u001b[35mDEBUG\u001b[0m] - connect_tcp.started host='huggingface.co' port=443 local_address=None timeout=10 socket_options=None\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,526\u001b[0m][\u001b[34mhttpcore.connection\u001b[0m][\u001b[35mDEBUG\u001b[0m] - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fde961fe150>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,526\u001b[0m][\u001b[34mhttpcore.connection\u001b[0m][\u001b[35mDEBUG\u001b[0m] - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fde19dbdac0> server_hostname='huggingface.co' timeout=10\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,578\u001b[0m][\u001b[34mhttpcore.connection\u001b[0m][\u001b[35mDEBUG\u001b[0m] - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fde1a0f1810>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,578\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - send_request_headers.started request=<Request [b'HEAD']>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,579\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - send_request_headers.complete\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,579\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - send_request_body.started request=<Request [b'HEAD']>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,580\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - send_request_body.complete\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:26,580\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - receive_response_headers.started request=<Request [b'HEAD']>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,085\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - receive_response_headers.complete return_value=(b'HTTP/1.1', 302, b'Found', [(b'Content-Type', b'text/plain; charset=utf-8'), (b'Content-Length', b'1315'), (b'Connection', b'keep-alive'), (b'Date', b'Wed, 18 Feb 2026 03:18:26 GMT'), (b'Location', b'https://cas-bridge.xethub.hf.co/xet-bridge-us/6397c0dddfdffcc725326e89/e6282dd6be728e3d5a75c840c738dac7d7b9e4382a00bd240ddd14a5736999cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260218%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260218T031826Z&X-Amz-Expires=3600&X-Amz-Signature=da71d14891a26db664bd24069ee3928478134ad796c57fc1ce73bd3144b9eb61&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1771388306&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc3MTM4ODMwNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82Mzk3YzBkZGRmZGZmY2M3MjUzMjZlODkvZTYyODJkZDZiZTcyOGUzZDVhNzVjODQwYzczOGRhYzdkN2I5ZTQzODJhMDBiZDI0MGRkZDE0YTU3MzY5OTljZioifV19&Signature=M8OzVCI9HDK24ANF1QatjMn6bajvrNy8JUvfiLnOiWhyv9GnURerKqwXDbBy9i4ztrJYscYmKwGyxwIJtPFyETQkB6OzmPZbqH8V6TH2THQDTMvgiqfN7iSiOqN0cRx2RFGqmy6lNq%7E9tfvMwHOhoJSD2taXqURirzUMaYojlgB0zwlWAZNFDcJHD84Ght-Tp7YWbaJcOewlDatYuqF-jA9%7Et6po4oj-2IACBSCOo2LbxcYFJmAQG6twOScexBuGQWN68Rixiuu1Gnexdok74n4ZTW1cChJ-1PSNXyjmY97NMPXUd3WFWeeYhhcdyM2las2blBDRSi2tSnvtTAn4GQ__&Key-Pair-Id=K2L8F4GPSG1IFC'), (b'X-Powered-By', b'huggingface-moon'), (b'X-Request-Id', b'Root=1-69952f82-0ceb8fa1253b79ce64f3cca2;7f48a6dd-d395-4392-b134-bc340d5f73c8'), (b'RateLimit', b'\"resolvers\";r=2999;t=178'), (b'RateLimit-Policy', b'\"fixed window\";\"resolvers\";q=3000;w=300'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'Access-Control-Max-Age', b'86400'), (b'Access-Control-Allow-Origin', b'https://huggingface.co'), (b'Vary', b'Origin, Accept'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Repo-Commit', b'625afb926af7ee851c12bdfef4520d7b2ba679d5'), (b'Accept-Ranges', b'bytes'), (b'X-Hub-Cache', b'MISS'), (b'X-Linked-Size', b'21355344'), (b'X-Linked-ETag', b'\"ca6fd4de70b4777557e7deca534935b5db289946d55c98c0b589c9c399771510\"'), (b'X-Xet-Hash', b'e6282dd6be728e3d5a75c840c738dac7d7b9e4382a00bd240ddd14a5736999cf'), (b'Link', b'<https://huggingface.co/api/models/timm/tf_efficientnet_b0.ns_jft_in1k/xet-read-token/625afb926af7ee851c12bdfef4520d7b2ba679d5>; rel=\"xet-auth\", <https://cas-server.xethub.hf.co/v1/reconstructions/e6282dd6be728e3d5a75c840c738dac7d7b9e4382a00bd240ddd14a5736999cf>; rel=\"xet-reconstruction-info\"'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f5a20b6dc3447fe14013d7d00f3345a4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'CCU50-P6'), (b'X-Amz-Cf-Id', b'IPk8OjTMpIskRtqHCCPy9jJUC5zqfLBrQdkSyxrJkZ5mmQwszSUbLA==')])\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,086\u001b[0m][\u001b[34mhttpx\u001b[0m][\u001b[32mINFO\u001b[0m] - HTTP Request: HEAD https://huggingface.co/timm/tf_efficientnet_b0.ns_jft_in1k/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,087\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - receive_response_body.started request=<Request [b'HEAD']>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,087\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - receive_response_body.complete\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,087\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - response_closed.started\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,087\u001b[0m][\u001b[34mhttpcore.http11\u001b[0m][\u001b[35mDEBUG\u001b[0m] - response_closed.complete\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,095\u001b[0m][\u001b[34mtimm.models._hub\u001b[0m][\u001b[32mINFO\u001b[0m] - [timm/tf_efficientnet_b0.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,144\u001b[0m][\u001b[34mtimm.models._builder\u001b[0m][\u001b[32mINFO\u001b[0m] - Missing keys (classifier.weight, classifier.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,154\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - [rank: 0] Instantiating callbacks...\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,155\u001b[0m][\u001b[34msrc.utils.instantiators\u001b[0m][\u001b[33mWARNING\u001b[0m] - [rank: 0] No callback configs found! Skipping..\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,155\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - [rank: 0] Instantiating loggers...\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,155\u001b[0m][\u001b[34msrc.utils.instantiators\u001b[0m][\u001b[33mWARNING\u001b[0m] - [rank: 0] No logger configs found! Skipping...\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,156\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - [rank: 0] Instantiating trainer <lightning.pytorch.trainer.Trainer>\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:27,225\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - [rank: 0] Starting training!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "üí° Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n",
      "‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m‚îÇ model          ‚îÇ EfficientNet      ‚îÇ  4.0 M ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m‚îÇ criterion      ‚îÇ BCEWithLogitsLoss ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m‚îÇ train_auroc    ‚îÇ BinaryAUROC       ‚îÇ      0 ‚îÇ train ‚îÇ     0 ÔøΩÔøΩ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m‚îÇ val_auroc      ‚îÇ BinaryAUROC       ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m‚îÇ test_auroc     ‚îÇ BinaryAUROC       ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m‚îÇ train_loss     ‚îÇ MeanMetric        ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m‚îÇ val_loss       ‚îÇ MeanMetric        ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m‚îÇ test_loss      ‚îÇ MeanMetric        ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îÇ\u001b[2m \u001b[0m\u001b[2m8\u001b[0m\u001b[2m \u001b[0m‚îÇ val_auroc_best ‚îÇ MaxMetric         ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\u001b[1mTrainable params\u001b[0m: 4.0 M                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 4.0 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 16                                      \n",
      "\u001b[1mModules in train mode\u001b[0m: 345                                                      \n",
      "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \n",
      "\u001b[2KEpoch 0/0  \u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 0/102 \u001b[2m0:00:00 ‚Ä¢ -:--:--\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mval/loss: 0.769   \u001b[0m-:--:--\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \n",
      "                                                              \u001b[3mval/auroc: 0.000  \u001b[0m\n",
      "                                                              \u001b[3mval/auroc_best:   \u001b[0m\n",
      "\u001b[2KEpoch 0/0  \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;98;6;224m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 33/102 \u001b[2m0:00:12 ‚Ä¢ 0:00:25\u001b[0m \u001b[2;4m2.85it/s\u001b[0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KEpoch 0/0  \u001b[38;2;98;6;224m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;98;6;224m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 33/102 \u001b[2m0:00:12 ‚Ä¢ 0:00:25\u001b[0m \u001b[2;4m2.85it/s\u001b[0m \n",
      "\u001b[?25h[\u001b[36m2026-02-18 09:18:50,846\u001b[0m][\u001b[34msrc.utils.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - [rank: 0] Output dir: /home/rudrashivm/code/dl/Skin-Cancer-Detection/logs\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:50,848\u001b[0m][\u001b[34mhttpcore.connection\u001b[0m][\u001b[35mDEBUG\u001b[0m] - close.started\u001b[0m\n",
      "[\u001b[36m2026-02-18 09:18:50,849\u001b[0m][\u001b[34mhttpcore.connection\u001b[0m][\u001b[35mDEBUG\u001b[0m] - close.complete\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# Override MODELS_TO_TRAIN, N_FOLDS, DATA_FRACTION, MAX_EPOCHS.\n",
    "# Models: efficientnet_b0, efficientnetv2_l, convnext_large,\n",
    "#         eva02_large, swin_large, convnext_tiny\n",
    "# ============================================================\n",
    "\n",
    "import warnings, os, glob\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "os.environ['LIGHTNING_DISABLE_UPGRADE_WARNINGS'] = '1'\n",
    "os.environ['LITMODULE_DISABLE_UPGRADE_WARNINGS'] = '1'\n",
    "\n",
    "# ---- OVERRIDE THESE ----\n",
    "MODELS_TO_TRAIN = [\"efficientnet_b0\"]\n",
    "N_FOLDS = 1        # set to 1 for a quick test\n",
    "DATA_FRACTION = 1\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "print(f\"Models: {MODELS_TO_TRAIN}\")\n",
    "print(f\"Folds: {N_FOLDS}  |  Data fraction: {DATA_FRACTION}  |  Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output dir:     {OUTPUT_DIR}\")\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    experiment = f\"isic_{model_name}\"\n",
    "    print(f\"\\n{'='*60}\\nMODEL: {model_name}  (experiment={experiment})\\n{'='*60}\")\n",
    "\n",
    "    for fold in range(N_FOLDS):\n",
    "        print(f\"\\n--- Fold {fold}/{N_FOLDS-1} ---\")\n",
    "\n",
    "        # Persisted output dir for Hydra logs, WandB saves, etc.\n",
    "        fold_output_dir = os.path.join(OUTPUT_DIR, model_name, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_output_dir, exist_ok=True)\n",
    "\n",
    "        # Persisted checkpoint dir\n",
    "        fold_ckpt_dir = os.path.join(CHECKPOINT_DIR, model_name, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_ckpt_dir, exist_ok=True)\n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        # Hydra override notes:\n",
    "        # - ++ forces override even when the config key uses ${} interpolation\n",
    "        # - Absolute paths with slashes MUST be shell-quoted so Hydra doesn't\n",
    "        #   interpret \"/\" as a config-group separator. We wrap them in single\n",
    "        #   quotes which the shell passes through verbatim to Hydra.\n",
    "        # - paths.log_dir is the root that output_dir and hydra.run.dir both\n",
    "        #   interpolate from, so overriding it alone would redirect everything.\n",
    "        #   We also set output_dir and dirpath explicitly for safety.\n",
    "        # -------------------------------------------------------------------\n",
    "        hydra_overrides = [\n",
    "            f\"data.data_dir={DATA_DIR}\",\n",
    "            f\"experiment={experiment}\",\n",
    "            f\"data.fold={fold}\",\n",
    "            f\"data.data_fraction={DATA_FRACTION}\",\n",
    "            f\"trainer.max_epochs={MAX_EPOCHS}\",\n",
    "            \"trainer.accelerator=gpu\",\n",
    "            \"extras.print_config=false\",\n",
    "            \"extras.enforce_tags=false\",\n",
    "            \"logger=wandb\",\n",
    "            # Quote paths so slashes are not misread as config group separators\n",
    "            f\"'++paths.log_dir={fold_output_dir}'\",\n",
    "            f\"'++paths.output_dir={fold_output_dir}'\",\n",
    "            f\"'++callbacks.model_checkpoint.dirpath={fold_ckpt_dir}'\",\n",
    "        ]\n",
    "\n",
    "        if PLATFORM == 'kaggle':\n",
    "            hydra_overrides += [\n",
    "                \"trainer.precision=16-mixed\",\n",
    "                \"data.num_workers=4\",\n",
    "                \"~callbacks.rich_progress_bar\",\n",
    "                \"~callbacks.model_summary\",\n",
    "            ]\n",
    "        elif PLATFORM == 'colab':\n",
    "            hydra_overrides += [\n",
    "                \"trainer.precision=16-mixed\",\n",
    "                \"data.num_workers=4\",\n",
    "            ]\n",
    "        else:  # local\n",
    "            hydra_overrides += [\"debug=default\"]\n",
    "\n",
    "        override_str = ' '.join(hydra_overrides)\n",
    "        cmd = f'python src/train.py {override_str}'\n",
    "        print(f\"Running:\\n  {cmd}\\n\")\n",
    "        ret = os.system(cmd)\n",
    "        print(f\"\\nExit code: {ret}\")\n",
    "\n",
    "        # ---- Checkpoint verification ----\n",
    "        # Check expected location\n",
    "        ckpts_found = glob.glob(os.path.join(fold_ckpt_dir, \"*.ckpt\"))\n",
    "        if ckpts_found:\n",
    "            print(f\"\\n\\u2705 Checkpoints at {fold_ckpt_dir}:\")\n",
    "            for c in ckpts_found:\n",
    "                print(f\"   {os.path.basename(c)}\")\n",
    "        else:\n",
    "            print(f\"\\n\\u26a0\\ufe0f  No .ckpt at expected path: {fold_ckpt_dir}\")\n",
    "            # Search everywhere under /kaggle/working/ to find where they landed\n",
    "            search_roots = [OUTPUT_DIR, CHECKPOINT_DIR, REPO_DIR]\n",
    "            all_ckpts = []\n",
    "            for root in search_roots:\n",
    "                all_ckpts += glob.glob(os.path.join(root, \"**\", \"*.ckpt\"), recursive=True)\n",
    "            if all_ckpts:\n",
    "                print(\"  Found .ckpt files at unexpected locations:\")\n",
    "                for c in set(all_ckpts):\n",
    "                    print(f\"   {c}\")\n",
    "            else:\n",
    "                print(\"  No .ckpt files found anywhere ‚Äî check training output above for errors\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\nTRAINING COMPLETE ‚Äî checkpoints in: {CHECKPOINT_DIR}\\n{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d59a87-826b-4135-b5a6-6b9ea189134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle auto-saves /kaggle/working/ when notebook finishes\n",
    "    # Just print where to find them\n",
    "    print(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "    print(\"Kaggle will persist these when you save the notebook.\")\n",
    "    \n",
    "    # Optional: commit results back to GitHub\n",
    "    push_to_github = False \n",
    "    if push_to_github:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        token    = secrets.get_secret('GITHUB_TOKEN')\n",
    "        email    = secrets.get_secret('GIT_EMAIL')\n",
    "        name     = secrets.get_secret('GIT_NAME')\n",
    "        \n",
    "        os.system(f'git config user.email \"{email}\"')\n",
    "        os.system(f'git config user.name  \"{name}\"')\n",
    "        os.system(f'git remote set-url origin https://{token}@github.com/YOU/REPO.git')\n",
    "        \n",
    "        os.system('git add outputs/ checkpoints/')\n",
    "        os.system('git commit -m \"Training results from Kaggle\"')\n",
    "        os.system('git push')\n",
    "        print(\"‚úÖ Pushed to GitHub\")\n",
    "    else:\n",
    "        print(f\"Results in /kaggle/working/ (auto-saved by Kaggle)\")\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Option A: Save to Google Drive (persists across sessions)\n",
    "    save_to_drive = True\n",
    "    if save_to_drive:\n",
    "        import shutil\n",
    "        drive_save_dir = '/content/drive/MyDrive/isic-results'\n",
    "        os.makedirs(drive_save_dir, exist_ok=True)\n",
    "        shutil.copytree(CHECKPOINT_DIR, f'{drive_save_dir}/checkpoints', dirs_exist_ok=True)\n",
    "        print(f\"‚úÖ Saved to Drive: {drive_save_dir}\")\n",
    "    \n",
    "    # Option B: Download to local machine\n",
    "    # from google.colab import files\n",
    "    # files.download(f'{CHECKPOINT_DIR}/best.ckpt')\n",
    "\n",
    "else:  # local\n",
    "    # Already on local machine, nothing special needed\n",
    "    print(f\"Results in: {OUTPUT_DIR}\")\n",
    "    print(f\"Checkpoints in: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e55bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE: Predict on individual images\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from src.models.isic_module import ISICLitModule\n",
    "from src.data.components.transforms import get_val_transforms\n",
    "from src.ensemble_predict import (\n",
    "    ensemble_predict, print_results,\n",
    "    find_best_checkpoint, load_model_from_checkpoint, get_model_img_size,\n",
    ")\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "# Option 1: Single model inference\n",
    "INFERENCE_MODEL = \"efficientnet_b0\"  # Model to use\n",
    "INFERENCE_FOLD = 0                    # Fold to use (or set to None for all folds)\n",
    "\n",
    "# Option 2: Ensemble inference (set to True to use all available models/folds)\n",
    "USE_ENSEMBLE = False\n",
    "ENSEMBLE_MODELS = [\"efficientnet_b0\"]  # Models for ensemble\n",
    "ENSEMBLE_STRATEGY = \"soft\"             # \"soft\" or \"hard_weighted\"\n",
    "\n",
    "# Image paths to predict on\n",
    "IMAGE_PATHS = [\n",
    "    # Add your image paths here, e.g.:\n",
    "    # \"/path/to/image1.jpg\",\n",
    "    # \"/path/to/image2.jpg\",\n",
    "]\n",
    "\n",
    "# ---- RUN PREDICTIONS ----\n",
    "if not IMAGE_PATHS:\n",
    "    print(\"\\u26a0\\ufe0f  No image paths specified. Add paths to IMAGE_PATHS list above.\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if USE_ENSEMBLE:\n",
    "        # Ensemble prediction across multiple models and folds\n",
    "        # img_size is auto-detected per model from the TIMM backbone config\n",
    "        print(f\"\\nRunning ensemble inference with models: {ENSEMBLE_MODELS}\")\n",
    "        print(f\"Strategy: {ENSEMBLE_STRATEGY}\")\n",
    "        results = ensemble_predict(\n",
    "            models=ENSEMBLE_MODELS,\n",
    "            checkpoint_dir=CHECKPOINT_DIR,\n",
    "            image_paths=IMAGE_PATHS,\n",
    "            img_size=None,  # auto-detect per model\n",
    "            device=device,\n",
    "            strategy=ENSEMBLE_STRATEGY,\n",
    "        )\n",
    "        print_results(results)\n",
    "        \n",
    "    else:\n",
    "        # Single model prediction\n",
    "        if INFERENCE_FOLD is not None:\n",
    "            # Use specific fold\n",
    "            model_dir = os.path.join(CHECKPOINT_DIR, INFERENCE_MODEL)\n",
    "            ckpt_path = find_best_checkpoint(model_dir, INFERENCE_FOLD)\n",
    "        else:\n",
    "            # Ensemble across all folds of one model\n",
    "            results = ensemble_predict(\n",
    "                models=[INFERENCE_MODEL],\n",
    "                checkpoint_dir=CHECKPOINT_DIR,\n",
    "                image_paths=IMAGE_PATHS,\n",
    "                img_size=None,  # auto-detect\n",
    "                device=device,\n",
    "            )\n",
    "            print_results(results)\n",
    "            ckpt_path = None  # Skip single model display below\n",
    "        \n",
    "        if ckpt_path:\n",
    "            print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "            model = load_model_from_checkpoint(ckpt_path, device)\n",
    "            \n",
    "            # Auto-detect image size from the model's TIMM backbone\n",
    "            img_size = get_model_img_size(model)\n",
    "            threshold = model.best_threshold.item()\n",
    "            print(f\"Auto-detected image size: {img_size}\")\n",
    "            print(f\"Optimal threshold: {threshold:.4f}\")\n",
    "            \n",
    "            transform = get_val_transforms(img_size)\n",
    "            \n",
    "            # Predict on each image\n",
    "            n_images = len(IMAGE_PATHS)\n",
    "            cols = min(n_images, 4)\n",
    "            rows = (n_images + cols - 1) // cols\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
    "            if n_images == 1:\n",
    "                axes = [axes]\n",
    "            else:\n",
    "                axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "            \n",
    "            for idx, img_path in enumerate(IMAGE_PATHS):\n",
    "                # Load and preprocess\n",
    "                image = cv2.imread(img_path)\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                transformed = transform(image=image_rgb)[\"image\"].unsqueeze(0).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    logit = model(transformed).squeeze()\n",
    "                    prob = torch.sigmoid(logit).item()\n",
    "                    pred = \"MALIGNANT\" if prob >= threshold else \"BENIGN\"\n",
    "                \n",
    "                # Display\n",
    "                axes[idx].imshow(image_rgb)\n",
    "                color = \"red\" if pred == \"MALIGNANT\" else \"green\"\n",
    "                axes[idx].set_title(\n",
    "                    f\"{pred}\\nProb: {prob:.4f} (thr: {threshold:.4f})\", \n",
    "                    color=color, fontsize=12, fontweight=\"bold\"\n",
    "                )\n",
    "                axes[idx].axis(\"off\")\n",
    "                print(f\"  {Path(img_path).name}: {pred} (prob={prob:.4f})\")\n",
    "            \n",
    "            # Hide unused axes\n",
    "            for idx in range(n_images, len(axes)):\n",
    "                axes[idx].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE: Generate submission from test-image.hdf5\n",
    "# ============================================================\n",
    "# This cell loads all test images + tabular features from the\n",
    "# competition HDF5 file and metadata CSV, runs ensemble inference\n",
    "# and produces a submission.csv matching the Kaggle format.\n",
    "# ============================================================\n",
    "\n",
    "import torch, os, h5py, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from src.models.isic_module import ISICLitModule\n",
    "from src.data.components.transforms import get_val_transforms\n",
    "from src.data.isic_datamodule import TABULAR_NUM_COLS, ISICDataModule\n",
    "from src.ensemble_predict import (\n",
    "    find_best_checkpoint, load_model_from_checkpoint,\n",
    "    get_model_img_size, discover_folds,\n",
    ")\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "SUBMISSION_MODELS = [\"efficientnet_b0\"]   # Models to ensemble\n",
    "ENSEMBLE_STRATEGY = \"soft\"                 # \"soft\" or \"hard_weighted\"\n",
    "BATCH_SIZE = 64\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "TEST_HDF5 = os.path.join(DATA_DIR, \"test-image.hdf5\")\n",
    "TEST_META = os.path.join(DATA_DIR, \"test-metadata.csv\")\n",
    "SAMPLE_SUB = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Verify files exist\n",
    "for path, label in [(TEST_HDF5, \"test HDF5\"), (TEST_META, \"test metadata\")]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\u2705 {label}: {path}\")\n",
    "    else:\n",
    "        print(f\"\\u26a0\\ufe0f  {label} NOT found: {path}\")\n",
    "\n",
    "# Load test IDs and metadata\n",
    "if os.path.exists(SAMPLE_SUB):\n",
    "    sample_df = pd.read_csv(SAMPLE_SUB)\n",
    "    isic_ids = sample_df[\"isic_id\"].tolist()\n",
    "elif os.path.exists(TEST_META):\n",
    "    sample_df = pd.read_csv(TEST_META, low_memory=False)\n",
    "    isic_ids = sample_df[\"isic_id\"].tolist()\n",
    "else:\n",
    "    raise FileNotFoundError(\"Neither sample_submission.csv nor test-metadata.csv found\")\n",
    "print(f\"\\nTest samples: {len(isic_ids)}\")\n",
    "\n",
    "# Encode tabular features from test metadata\n",
    "test_meta_df = pd.read_csv(TEST_META, low_memory=False)\n",
    "test_meta_df = test_meta_df.set_index('isic_id').loc[isic_ids].reset_index()\n",
    "\n",
    "# Use the same encoding as ISICDataModule\n",
    "dm_dummy = ISICDataModule.__new__(ISICDataModule)\n",
    "test_tabular_raw = dm_dummy._encode_tabular(test_meta_df)\n",
    "# Self-standardize (approximate; ideal: use training stats)\n",
    "tab_mean = test_tabular_raw.mean(axis=0)\n",
    "tab_std = test_tabular_raw.std(axis=0)\n",
    "tab_std[tab_std < 1e-7] = 1.0\n",
    "test_tabular = (test_tabular_raw - tab_mean) / tab_std\n",
    "print(f\"Tabular features: {test_tabular.shape}\")\n",
    "\n",
    "# ---- LOAD ALL MODELS/FOLDS ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nDevice: {device}\")\n",
    "model_configs = []\n",
    "\n",
    "for model_name in SUBMISSION_MODELS:\n",
    "    model_dir = os.path.join(CHECKPOINT_DIR, model_name)\n",
    "    folds = discover_folds(model_dir)\n",
    "    if not folds:\n",
    "        print(f\"\\u26a0\\ufe0f  No folds for {model_name}\")\n",
    "        continue\n",
    "    print(f\"\\n{model_name}: {len(folds)} folds\")\n",
    "    for fold in folds:\n",
    "        ckpt_path = find_best_checkpoint(model_dir, fold)\n",
    "        if ckpt_path is None:\n",
    "            continue\n",
    "        print(f\"  Fold {fold}: {os.path.basename(ckpt_path)}\")\n",
    "        model = load_model_from_checkpoint(ckpt_path, device)\n",
    "        img_size = get_model_img_size(model)\n",
    "        threshold = model.best_threshold.item()\n",
    "        auroc = model.best_auroc.item() if hasattr(model, \"best_auroc\") else 0.5\n",
    "        uses_tabular = hasattr(model, 'fusion_head') and model.fusion_head is not None\n",
    "        print(f\"    img_size={img_size}, threshold={threshold:.4f}, auroc={auroc:.4f}, tabular={uses_tabular}\")\n",
    "        model_configs.append((model, img_size, threshold, auroc, uses_tabular))\n",
    "\n",
    "print(f\"\\nTotal model-folds: {len(model_configs)}\")\n",
    "\n",
    "# ---- RUN INFERENCE ON HDF5 ----\n",
    "hdf5 = h5py.File(TEST_HDF5, \"r\")\n",
    "all_probs, all_thresholds, all_aurocs = [], [], []\n",
    "\n",
    "for idx, (model, img_size, threshold, auroc, uses_tabular) in enumerate(model_configs):\n",
    "    print(f\"\\nInference {idx+1}/{len(model_configs)} (img_size={img_size}, tabular={uses_tabular})...\")\n",
    "    transform = get_val_transforms(img_size)\n",
    "    probs = []\n",
    "    for batch_start in tqdm(range(0, len(isic_ids), BATCH_SIZE), desc=\"Batches\"):\n",
    "        batch_ids = isic_ids[batch_start : batch_start + BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        for isic_id in batch_ids:\n",
    "            img_bytes = hdf5[isic_id][()]\n",
    "            img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = transform(image=img)[\"image\"]\n",
    "            batch_images.append(img)\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        if uses_tabular:\n",
    "            batch_tab = torch.tensor(\n",
    "                test_tabular[batch_start:batch_start+len(batch_ids)], dtype=torch.float32\n",
    "            ).to(device)\n",
    "        else:\n",
    "            batch_tab = None\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_tensor, batch_tab).squeeze(1)\n",
    "            batch_probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        probs.extend(batch_probs.tolist())\n",
    "    all_probs.append(np.array(probs))\n",
    "    all_thresholds.append(threshold)\n",
    "    all_aurocs.append(auroc)\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "hdf5.close()\n",
    "\n",
    "# ---- ENSEMBLE ----\n",
    "all_probs = np.array(all_probs)\n",
    "if ENSEMBLE_STRATEGY == \"hard_weighted\":\n",
    "    weights = np.array(all_aurocs)\n",
    "    weights = weights / weights.sum() if weights.sum() > 0 else np.ones_like(weights) / len(weights)\n",
    "    hard = np.array([(p >= t).astype(float) for p, t in zip(all_probs, all_thresholds)])\n",
    "    final_probs = np.average(hard, axis=0, weights=weights)\n",
    "    final_threshold = 0.5\n",
    "else:\n",
    "    final_probs = np.mean(all_probs, axis=0)\n",
    "    final_threshold = np.mean(all_thresholds)\n",
    "\n",
    "final_preds = (final_probs >= final_threshold).astype(int)\n",
    "\n",
    "# ---- GENERATE SUBMISSION ----\n",
    "submission = pd.DataFrame({\"isic_id\": isic_ids, \"target\": final_preds})\n",
    "submission.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUBMISSION: {OUTPUT_CSV}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Samples: {len(submission)}\")\n",
    "print(f\"  Malignant: {final_preds.sum()} ({final_preds.mean()*100:.2f}%)\")\n",
    "print(f\"  Benign: {len(final_preds) - final_preds.sum()}\")\n",
    "print(f\"  Strategy: {ENSEMBLE_STRATEGY}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5faed-65f8-43f3-b79f-f9f145b8f534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
