{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4521c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: local\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def detect_platform():\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        return 'kaggle'\n",
    "    elif 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "PLATFORM = detect_platform()\n",
    "print(f\"Running on: {PLATFORM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de978d8-7eec-4a63-8699-8e6e7be37e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo directory: /home/rudrashivm/code/dl/Skin-Cancer-Detection\n",
      "Installing dependency packages...\n",
      "Installing the current project as an editable Python package...\n",
      "âœ… Setup complete\n"
     ]
    }
   ],
   "source": [
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle: repo must be attached as a dataset or cloned\n",
    "\n",
    "    os.system('git clone https://github.com/RudraShivm/Skin-Cancer-Detection.git /kaggle/working/skin-cancer-detection')\n",
    "    REPO_DIR = '/kaggle/working/skin-cancer-detection'\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Colab: clone from GitHub\n",
    "    # For private repo, use Colab secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        token = userdata.get('GITHUB_TOKEN')\n",
    "        clone_url = f'https://{token}@github.com/RudraShivm/Skin-Cancer-Detection.git'\n",
    "    except Exception:\n",
    "        clone_url = 'https://github.com/RudraShivm/Skin-Cancer-Detection.git'  # public\n",
    "    \n",
    "    os.system(f'git clone {clone_url} /content/skin-cancer-detection')\n",
    "    REPO_DIR = '/content/skin-cancer-detection'\n",
    "\n",
    "else:  # local\n",
    "    import rootutils\n",
    "    REPO_DIR = str(rootutils.find_root(search_from='.', indicator='.project-root'))\n",
    "\n",
    "print(f\"Repo directory: {REPO_DIR}\")\n",
    "\n",
    "# Change to repo directory and install\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Installing dependency packages...\")\n",
    "os.system('pip install -r requirements.txt -q')  # â† install packages\n",
    "print(f\"Installing the current project as an editable Python package...\")\n",
    "os.system('pip install -e . -q')                 # â† make src/ importable\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8016be7d-149b-483b-a215-b6d0392c537e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "WANDB_API_KEY invalid: API key may only contain the letters A-Z, digits and underscores.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m                     os.environ[key.strip()] = val.strip()\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33menv::PROJECT_ROOT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ.get(\u001b[33m'\u001b[39m\u001b[33mPROJECT_ROOT\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:101\u001b[39m, in \u001b[36mlogin\u001b[39m\u001b[34m(key, relogin, host, force, timeout, verify, referrer, anonymous)\u001b[39m\n\u001b[32m     94\u001b[39m     host = host.rstrip(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m _update_system_settings(\n\u001b[32m     97\u001b[39m     global_settings.read_system_settings(),\n\u001b[32m     98\u001b[39m     host=host,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m logged_in, _ = \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logged_in\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:190\u001b[39m, in \u001b[36m_login\u001b[39m\u001b[34m(key, relogin, host, force, timeout, verify, referrer, update_api_key, _silent)\u001b[39m\n\u001b[32m    182\u001b[39m     auth: wbauth.Auth | \u001b[38;5;28;01mNone\u001b[39;00m = _use_explicit_key(\n\u001b[32m    183\u001b[39m         key,\n\u001b[32m    184\u001b[39m         host=host_url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m         silent=_silent,\n\u001b[32m    188\u001b[39m     )\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     auth = \u001b[43m_find_or_prompt_for_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(auth, wbauth.AuthApiKey):\n\u001b[32m    200\u001b[39m     _verify_login(key=auth.api_key, base_url=auth.host.url)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:266\u001b[39m, in \u001b[36m_find_or_prompt_for_key\u001b[39m\u001b[34m(settings, host, force, relogin, referrer, input_timeout)\u001b[39m\n\u001b[32m    263\u001b[39m auth: wbauth.Auth | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     auth = \u001b[43mwbauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauthenticate_session\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwandb.login()\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_offline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_create\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m    277\u001b[39m     timed_out = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/lib/wbauth/authenticate.py:127\u001b[39m, in \u001b[36mauthenticate_session\u001b[39m\u001b[34m(host, source, no_offline, no_create, input_timeout, referrer, relogin)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relogin \u001b[38;5;129;01mand\u001b[39;00m (auth := session_credentials(host=host)):\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m auth\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relogin \u001b[38;5;129;01mand\u001b[39;00m (auth := \u001b[43m_use_system_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m auth\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/lib/wbauth/authenticate.py:188\u001b[39m, in \u001b[36m_use_system_auth\u001b[39m\u001b[34m(host, source)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_use_system_auth\u001b[39m(*, host: HostUrl, source: \u001b[38;5;28mstr\u001b[39m) -> Auth | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load (or reload) session credentials from external sources.\u001b[39;00m\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m \u001b[33;03m    Loads credentials from environment variables or the .netrc file.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m \u001b[33;03m        The new credentials, if any.\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    187\u001b[39m     auth = (\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m         \u001b[43m_try_env_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    189\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m wbnetrc.read_netrc_auth_with_source(host=host)\n\u001b[32m    190\u001b[39m     )\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _session_auth_lock:\n\u001b[32m    193\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m auth:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/skin-cancer-detection/lib/python3.11/site-packages/wandb/sdk/lib/wbauth/authenticate.py:225\u001b[39m, in \u001b[36m_try_env_auth\u001b[39m\u001b[34m(host)\u001b[39m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m AuthWithSource(\n\u001b[32m    221\u001b[39m             auth=AuthApiKey(host=host, api_key=api_key),\n\u001b[32m    222\u001b[39m             source=env.API_KEY,\n\u001b[32m    223\u001b[39m         )\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m AuthenticationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv.API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m invalid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m identity_token_file:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AuthWithSource(\n\u001b[32m    229\u001b[39m         auth=AuthIdentityTokenFile(\n\u001b[32m    230\u001b[39m             host=host,\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m         source=env.IDENTITY_TOKEN_FILE,\n\u001b[32m    235\u001b[39m     )\n",
      "\u001b[31mAuthenticationError\u001b[39m: WANDB_API_KEY invalid: API key may only contain the letters A-Z, digits and underscores."
     ]
    }
   ],
   "source": [
    "# PROJECT_ROOT is needed by rootutils in train.py\n",
    "os.environ['PROJECT_ROOT'] = REPO_DIR\n",
    "\n",
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle secrets (set in Kaggle â†’ Add-ons â†’ Secrets)\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        os.environ['WANDB_API_KEY'] = secrets.get_secret('WANDB_API_KEY')\n",
    "    except Exception:\n",
    "        pass  # Secrets not available, skip\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Colab secrets (set in Colab â†’ ðŸ”‘ sidebar)\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "else:  # local\n",
    "    # Local: load from .env file if it exists\n",
    "    env_file = os.path.join(REPO_DIR, '.env')\n",
    "    if os.path.exists(env_file):\n",
    "        with open(env_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '=' in line:\n",
    "                    key, val = line.split('=', 1)\n",
    "                    os.environ[key.strip()] = val.strip()\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "print(f\"env::PROJECT_ROOT: {os.environ.get('PROJECT_ROOT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6311327a-b014-439e-b705-0ba6c4393082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:        /home/rudrashivm/code/dl/Skin-Cancer-Detection/data/isic-2024-challenge\n",
      "Checkpoints: /home/rudrashivm/code/dl/Skin-Cancer-Detection/checkpoints\n",
      "Outputs:     /home/rudrashivm/code/dl/Skin-Cancer-Detection/outputs\n",
      "âœ… Data found: ['sample_submission.csv', 'test-image.hdf5', 'test-metadata.csv', 'train-image.hdf5', 'train-image']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = None\n",
    "CHECKPOINT_DIR = None\n",
    "OUTPUT_DIR = None\n",
    "\n",
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle mounts competition data at /kaggle/input/\n",
    "    DATA_DIR = '/kaggle/input/competitions/isic-2024-challenge'\n",
    "    \n",
    "    # Outputs go to /kaggle/working/ (this is what gets saved)\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    OUTPUT_DIR = '/kaggle/working/outputs'\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Option A: Data in Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    DATA_DIR = '/content/drive/MyDrive/isic-2024-challenge'\n",
    "    \n",
    "    # Option B: Data downloaded to Colab's ephemeral disk\n",
    "    # DATA_DIR = '/content/isic-2024-challenge'\n",
    "    \n",
    "    CHECKPOINT_DIR = f'{REPO_DIR}/checkpoints'\n",
    "    OUTPUT_DIR = f'{REPO_DIR}/outputs'\n",
    "\n",
    "else:  # local\n",
    "    DATA_DIR = os.path.join(REPO_DIR, 'data', 'isic-2024-challenge')\n",
    "    CHECKPOINT_DIR = os.path.join(REPO_DIR, 'checkpoints')\n",
    "    OUTPUT_DIR = os.path.join(REPO_DIR, 'outputs')\n",
    "\n",
    "# Create dirs if needed\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data:        {DATA_DIR}\")\n",
    "print(f\"Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"Outputs:     {OUTPUT_DIR}\")\n",
    "\n",
    "# Verify data exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"âœ… Data found: {os.listdir(DATA_DIR)[:5]}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Data NOT found at {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdadacf-9365-44ef-9c24-34baf17fa475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# Override MODELS_TO_TRAIN, N_FOLDS, DATA_FRACTION, MAX_EPOCHS.\n",
    "# Models: efficientnet_b0, efficientnetv2_l, convnext_large,\n",
    "#         eva02_large, eva02_small, swin_large, convnext_tiny,\n",
    "#         mobilenet_v3\n",
    "# ============================================================\n",
    "\n",
    "import warnings, os, glob\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "os.environ['LIGHTNING_DISABLE_UPGRADE_WARNINGS'] = '1'\n",
    "os.environ['LITMODULE_DISABLE_UPGRADE_WARNINGS'] = '1'\n",
    "\n",
    "# ---- OVERRIDE THESE ----\n",
    "MODELS_TO_TRAIN = [\"efficientnet_b0\"]\n",
    "N_FOLDS = 1        # set to 1 for a quick test\n",
    "DATA_FRACTION = 1\n",
    "MAX_EPOCHS = 10\n",
    "SEED = 42           # change seed for multi-seed ensemble training\n",
    "\n",
    "print(f\"Models: {MODELS_TO_TRAIN}\")\n",
    "print(f\"Folds: {N_FOLDS}  |  Data fraction: {DATA_FRACTION}  |  Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"Seed: {SEED}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output dir:     {OUTPUT_DIR}\")\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    experiment = f\"isic_{model_name}\"\n",
    "    print(f\"\\n{'='*60}\\nMODEL: {model_name}  (experiment={experiment})\\n{'='*60}\")\n",
    "\n",
    "    for fold in range(N_FOLDS):\n",
    "        print(f\"\\n--- Fold {fold}/{N_FOLDS-1} ---\")\n",
    "\n",
    "        # Persisted output dir for Hydra logs, WandB saves, etc.\n",
    "        fold_output_dir = os.path.join(OUTPUT_DIR, model_name, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_output_dir, exist_ok=True)\n",
    "\n",
    "        # Persisted checkpoint dir\n",
    "        fold_ckpt_dir = os.path.join(CHECKPOINT_DIR, model_name, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_ckpt_dir, exist_ok=True)\n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        # Hydra override notes:\n",
    "        # - ++ forces override even when the config key uses ${} interpolation\n",
    "        # - Absolute paths with slashes MUST be shell-quoted so Hydra doesn't\n",
    "        #   interpret \"/\" as a config-group separator. We wrap them in single\n",
    "        #   quotes which the shell passes through verbatim to Hydra.\n",
    "        # - paths.log_dir is the root that output_dir and hydra.run.dir both\n",
    "        #   interpolate from, so overriding it alone would redirect everything.\n",
    "        #   We also set output_dir and dirpath explicitly for safety.\n",
    "        # -------------------------------------------------------------------\n",
    "        hydra_overrides = [\n",
    "            f\"data.data_dir={DATA_DIR}\",\n",
    "            f\"experiment={experiment}\",\n",
    "            f\"data.fold={fold}\",\n",
    "            f\"data.data_fraction={DATA_FRACTION}\",\n",
    "            f\"trainer.max_epochs={MAX_EPOCHS}\",\n",
    "            f\"seed={SEED}\",\n",
    "            \"trainer.accelerator=gpu\",\n",
    "            \"extras.print_config=false\",\n",
    "            \"extras.enforce_tags=false\",\n",
    "            \"logger=wandb\",\n",
    "            # Quote paths so slashes are not misread as config group separators\n",
    "            f\"'++paths.log_dir={fold_output_dir}'\",\n",
    "            f\"'++paths.output_dir={fold_output_dir}'\",\n",
    "            f\"'++callbacks.model_checkpoint.dirpath={fold_ckpt_dir}'\",\n",
    "        ]\n",
    "\n",
    "        if PLATFORM == 'kaggle':\n",
    "            hydra_overrides += [\n",
    "                \"trainer.precision=16-mixed\",\n",
    "                \"data.num_workers=4\",\n",
    "                \"~callbacks.rich_progress_bar\",\n",
    "                \"~callbacks.model_summary\",\n",
    "            ]\n",
    "        elif PLATFORM == 'colab':\n",
    "            hydra_overrides += [\n",
    "                \"trainer.precision=16-mixed\",\n",
    "                \"data.num_workers=4\",\n",
    "            ]\n",
    "        else:  # local\n",
    "            hydra_overrides += [\"debug=default\"]\n",
    "\n",
    "        override_str = ' '.join(hydra_overrides)\n",
    "        cmd = f'python src/train.py {override_str}'\n",
    "        print(f\"Running:\\n  {cmd}\\n\")\n",
    "        ret = os.system(cmd)\n",
    "        print(f\"\\nExit code: {ret}\")\n",
    "\n",
    "        # ---- Checkpoint verification ----\n",
    "        # Check expected location\n",
    "        ckpts_found = glob.glob(os.path.join(fold_ckpt_dir, \"*.ckpt\"))\n",
    "        if ckpts_found:\n",
    "            print(f\"\\n\\u2705 Checkpoints at {fold_ckpt_dir}:\")\n",
    "            for c in ckpts_found:\n",
    "                print(f\"   {os.path.basename(c)}\")\n",
    "        else:\n",
    "            print(f\"\\n\\u26a0\\ufe0f  No .ckpt at expected path: {fold_ckpt_dir}\")\n",
    "            # Search everywhere under /kaggle/working/ to find where they landed\n",
    "            search_roots = [OUTPUT_DIR, CHECKPOINT_DIR, REPO_DIR]\n",
    "            all_ckpts = []\n",
    "            for root in search_roots:\n",
    "                all_ckpts += glob.glob(os.path.join(root, \"**\", \"*.ckpt\"), recursive=True)\n",
    "            if all_ckpts:\n",
    "                print(\"  Found .ckpt files at unexpected locations:\")\n",
    "                for c in set(all_ckpts):\n",
    "                    print(f\"   {c}\")\n",
    "            else:\n",
    "                print(\"  No .ckpt files found anywhere â€” check training output above for errors\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\nTRAINING COMPLETE â€” checkpoints in: {CHECKPOINT_DIR}\\n{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d59a87-826b-4135-b5a6-6b9ea189134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle auto-saves /kaggle/working/ when notebook finishes\n",
    "    # Just print where to find them\n",
    "    print(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "    print(\"Kaggle will persist these when you save the notebook.\")\n",
    "    \n",
    "    # Optional: commit results back to GitHub\n",
    "    push_to_github = False \n",
    "    if push_to_github:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        token    = secrets.get_secret('GITHUB_TOKEN')\n",
    "        email    = secrets.get_secret('GIT_EMAIL')\n",
    "        name     = secrets.get_secret('GIT_NAME')\n",
    "        \n",
    "        os.system(f'git config user.email \"{email}\"')\n",
    "        os.system(f'git config user.name  \"{name}\"')\n",
    "        os.system(f'git remote set-url origin https://{token}@github.com/YOU/REPO.git')\n",
    "        \n",
    "        os.system('git add outputs/ checkpoints/')\n",
    "        os.system('git commit -m \"Training results from Kaggle\"')\n",
    "        os.system('git push')\n",
    "        print(\"âœ… Pushed to GitHub\")\n",
    "    else:\n",
    "        print(f\"Results in /kaggle/working/ (auto-saved by Kaggle)\")\n",
    "\n",
    "elif PLATFORM == 'colab':\n",
    "    # Option A: Save to Google Drive (persists across sessions)\n",
    "    save_to_drive = True\n",
    "    if save_to_drive:\n",
    "        import shutil\n",
    "        drive_save_dir = '/content/drive/MyDrive/isic-results'\n",
    "        os.makedirs(drive_save_dir, exist_ok=True)\n",
    "        shutil.copytree(CHECKPOINT_DIR, f'{drive_save_dir}/checkpoints', dirs_exist_ok=True)\n",
    "        print(f\"âœ… Saved to Drive: {drive_save_dir}\")\n",
    "    \n",
    "    # Option B: Download to local machine\n",
    "    # from google.colab import files\n",
    "    # files.download(f'{CHECKPOINT_DIR}/best.ckpt')\n",
    "\n",
    "else:  # local\n",
    "    # Already on local machine, nothing special needed\n",
    "    print(f\"Results in: {OUTPUT_DIR}\")\n",
    "    print(f\"Checkpoints in: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e55bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE: Predict on individual images\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from src.models.isic_module import ISICLitModule\n",
    "from src.data.components.transforms import get_val_transforms\n",
    "from src.ensemble_predict import (\n",
    "    ensemble_predict, print_results,\n",
    "    find_best_checkpoint, load_model_from_checkpoint, get_model_img_size,\n",
    ")\n",
    "\n",
    "def compute_confidence(prob, threshold):\n",
    "    \"\"\"Compute confidence score using log-odds distance.\n",
    "    \n",
    "    Returns (confidence_pct, risk_label) where confidence_pct is 0-100\n",
    "    and risk_label is a human-readable string.\n",
    "    \"\"\"\n",
    "    eps = 1e-7\n",
    "    p = max(eps, min(prob, 1 - eps))\n",
    "    t = max(eps, min(threshold, 1 - eps))\n",
    "    logit_p = math.log(p / (1 - p))\n",
    "    logit_t = math.log(t / (1 - t))\n",
    "    distance = abs(logit_p - logit_t)\n",
    "    confidence_pct = (1 - math.exp(-distance / 2.0)) * 100\n",
    "    if prob >= threshold:\n",
    "        if distance > 3.0: return confidence_pct, 'High Risk'\n",
    "        elif distance > 1.5: return confidence_pct, 'Moderate Risk'\n",
    "        else: return confidence_pct, 'Low Risk (borderline)'\n",
    "    else:\n",
    "        if distance > 3.0: return confidence_pct, 'Very Likely Benign'\n",
    "        elif distance > 1.5: return confidence_pct, 'Likely Benign'\n",
    "        else: return confidence_pct, 'Uncertain (borderline)'\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "# Option 1: Single model inference\n",
    "INFERENCE_MODEL = \"efficientnet_b0\"  # Model to use\n",
    "INFERENCE_FOLD = 0                    # Fold to use (or set to None for all folds)\n",
    "\n",
    "# Option 2: Ensemble inference (set to True to use all available models/folds)\n",
    "USE_ENSEMBLE = False\n",
    "ENSEMBLE_MODELS = [\"efficientnet_b0\"]  # Models for ensemble\n",
    "ENSEMBLE_STRATEGY = \"soft\"             # \"soft\" or \"hard_weighted\"\n",
    "\n",
    "# Image paths to predict on\n",
    "IMAGE_PATHS = [\n",
    "    # Add your image paths here, e.g.:\n",
    "    # \"/path/to/image1.jpg\",\n",
    "    # \"/path/to/image2.jpg\",\n",
    "]\n",
    "\n",
    "# ---- RUN PREDICTIONS ----\n",
    "if not IMAGE_PATHS:\n",
    "    print(\"\\u26a0\\ufe0f  No image paths specified. Add paths to IMAGE_PATHS list above.\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if USE_ENSEMBLE:\n",
    "        # Ensemble prediction across multiple models and folds\n",
    "        # img_size is auto-detected per model from the TIMM backbone config\n",
    "        print(f\"\\nRunning ensemble inference with models: {ENSEMBLE_MODELS}\")\n",
    "        print(f\"Strategy: {ENSEMBLE_STRATEGY}\")\n",
    "        results = ensemble_predict(\n",
    "            models=ENSEMBLE_MODELS,\n",
    "            checkpoint_dir=CHECKPOINT_DIR,\n",
    "            image_paths=IMAGE_PATHS,\n",
    "            img_size=None,  # auto-detect per model\n",
    "            device=device,\n",
    "            strategy=ENSEMBLE_STRATEGY,\n",
    "        )\n",
    "        print_results(results)\n",
    "        \n",
    "    else:\n",
    "        # Single model prediction\n",
    "        if INFERENCE_FOLD is not None:\n",
    "            # Use specific fold\n",
    "            model_dir = os.path.join(CHECKPOINT_DIR, INFERENCE_MODEL)\n",
    "            ckpt_path = find_best_checkpoint(model_dir, INFERENCE_FOLD)\n",
    "        else:\n",
    "            # Ensemble across all folds of one model\n",
    "            results = ensemble_predict(\n",
    "                models=[INFERENCE_MODEL],\n",
    "                checkpoint_dir=CHECKPOINT_DIR,\n",
    "                image_paths=IMAGE_PATHS,\n",
    "                img_size=None,  # auto-detect\n",
    "                device=device,\n",
    "            )\n",
    "            print_results(results)\n",
    "            ckpt_path = None  # Skip single model display below\n",
    "        \n",
    "        if ckpt_path:\n",
    "            print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "            model = load_model_from_checkpoint(ckpt_path, device)\n",
    "            \n",
    "            # Auto-detect image size from the model's TIMM backbone\n",
    "            img_size = get_model_img_size(model)\n",
    "            threshold = model.best_threshold.item()\n",
    "            print(f\"Auto-detected image size: {img_size}\")\n",
    "            print(f\"Optimal threshold: {threshold:.4f}\")\n",
    "            \n",
    "            transform = get_val_transforms(img_size)\n",
    "            \n",
    "            # Predict on each image\n",
    "            n_images = len(IMAGE_PATHS)\n",
    "            cols = min(n_images, 4)\n",
    "            rows = (n_images + cols - 1) // cols\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
    "            if n_images == 1:\n",
    "                axes = [axes]\n",
    "            else:\n",
    "                axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "            \n",
    "            for idx, img_path in enumerate(IMAGE_PATHS):\n",
    "                # Load and preprocess\n",
    "                image = cv2.imread(img_path)\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                transformed = transform(image=image_rgb)[\"image\"].unsqueeze(0).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    logit = model(transformed).squeeze()\n",
    "                    prob = torch.sigmoid(logit).item()\n",
    "                    pred = \"MALIGNANT\" if prob >= threshold else \"BENIGN\"\n",
    "                    conf_pct, risk_label = compute_confidence(prob, threshold)\n",
    "                \n",
    "                # Display\n",
    "                axes[idx].imshow(image_rgb)\n",
    "                color = \"red\" if pred == \"MALIGNANT\" else \"green\"\n",
    "                axes[idx].set_title(\n",
    "                    f\"{pred} ({risk_label})\\nProb: {prob:.4f} (thr: {threshold:.4f})\\nConfidence: {conf_pct:.0f}%\", \n",
    "                    color=color, fontsize=11, fontweight=\"bold\"\n",
    "                )\n",
    "                axes[idx].axis(\"off\")\n",
    "                print(f\"  {Path(img_path).name}: {pred} (prob={prob:.4f}, conf={conf_pct:.0f}% \\u2014 {risk_label})\")\n",
    "            \n",
    "            # Hide unused axes\n",
    "            for idx in range(n_images, len(axes)):\n",
    "                axes[idx].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE: Generate submission from test-image.hdf5\n",
    "# ============================================================\n",
    "# This cell loads all test images + tabular features from the\n",
    "# competition HDF5 file and metadata CSV, runs ensemble inference\n",
    "# and produces a submission.csv matching the Kaggle format.\n",
    "# ============================================================\n",
    "\n",
    "import torch, os, h5py, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from src.models.isic_module import ISICLitModule\n",
    "from src.data.components.transforms import get_val_transforms\n",
    "from src.data.isic_datamodule import TABULAR_NUM_COLS, ISICDataModule\n",
    "from src.ensemble_predict import (\n",
    "    find_best_checkpoint, load_model_from_checkpoint,\n",
    "    get_model_img_size, discover_folds,\n",
    ")\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "SUBMISSION_MODELS = [\"efficientnet_b0\"]   # Models to ensemble\n",
    "ENSEMBLE_STRATEGY = \"soft\"                 # \"soft\" or \"hard_weighted\"\n",
    "BATCH_SIZE = 64\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "TEST_HDF5 = os.path.join(DATA_DIR, \"test-image.hdf5\")\n",
    "TEST_META = os.path.join(DATA_DIR, \"test-metadata.csv\")\n",
    "SAMPLE_SUB = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Verify files exist\n",
    "for path, label in [(TEST_HDF5, \"test HDF5\"), (TEST_META, \"test metadata\")]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\u2705 {label}: {path}\")\n",
    "    else:\n",
    "        print(f\"\\u26a0\\ufe0f  {label} NOT found: {path}\")\n",
    "\n",
    "# Load test IDs and metadata\n",
    "if os.path.exists(SAMPLE_SUB):\n",
    "    sample_df = pd.read_csv(SAMPLE_SUB)\n",
    "    isic_ids = sample_df[\"isic_id\"].tolist()\n",
    "elif os.path.exists(TEST_META):\n",
    "    sample_df = pd.read_csv(TEST_META, low_memory=False)\n",
    "    isic_ids = sample_df[\"isic_id\"].tolist()\n",
    "else:\n",
    "    raise FileNotFoundError(\"Neither sample_submission.csv nor test-metadata.csv found\")\n",
    "print(f\"\\nTest samples: {len(isic_ids)}\")\n",
    "\n",
    "# Encode tabular features from test metadata\n",
    "test_meta_df = pd.read_csv(TEST_META, low_memory=False)\n",
    "test_meta_df = test_meta_df.set_index('isic_id').loc[isic_ids].reset_index()\n",
    "\n",
    "# Use the same encoding as ISICDataModule\n",
    "dm_dummy = ISICDataModule.__new__(ISICDataModule)\n",
    "test_tabular_raw = dm_dummy._encode_tabular(test_meta_df)\n",
    "dm_dummy._tab_mean = test_tabular_raw.mean(axis=0)\n",
    "dm_dummy._tab_std = test_tabular_raw.std(axis=0)\n",
    "dm_dummy._tab_std[dm_dummy._tab_std < 1e-7] = 1.0\n",
    "test_tabular = dm_dummy._patient_standardize(test_tabular_raw, test_meta_df['patient_id'].values)\n",
    "print(f\"Tabular features: {test_tabular.shape}\")\n",
    "\n",
    "# ---- LOAD ALL MODELS/FOLDS ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nDevice: {device}\")\n",
    "model_configs = []\n",
    "\n",
    "for model_name in SUBMISSION_MODELS:\n",
    "    model_dir = os.path.join(CHECKPOINT_DIR, model_name)\n",
    "    folds = discover_folds(model_dir)\n",
    "    if not folds:\n",
    "        print(f\"\\u26a0\\ufe0f  No folds for {model_name}\")\n",
    "        continue\n",
    "    print(f\"\\n{model_name}: {len(folds)} folds\")\n",
    "    for fold in folds:\n",
    "        ckpt_path = find_best_checkpoint(model_dir, fold)\n",
    "        if ckpt_path is None:\n",
    "            continue\n",
    "        print(f\"  Fold {fold}: {os.path.basename(ckpt_path)}\")\n",
    "        model = load_model_from_checkpoint(ckpt_path, device)\n",
    "        img_size = get_model_img_size(model)\n",
    "        threshold = model.best_threshold.item()\n",
    "        auroc = model.best_auroc.item() if hasattr(model, \"best_auroc\") else 0.5\n",
    "        uses_tabular = hasattr(model, 'fusion_head') and model.fusion_head is not None\n",
    "        print(f\"    img_size={img_size}, threshold={threshold:.4f}, auroc={auroc:.4f}, tabular={uses_tabular}\")\n",
    "        model_configs.append((model, img_size, threshold, auroc, uses_tabular))\n",
    "\n",
    "print(f\"\\nTotal model-folds: {len(model_configs)}\")\n",
    "\n",
    "# ---- RUN INFERENCE ON HDF5 ----\n",
    "hdf5 = h5py.File(TEST_HDF5, \"r\")\n",
    "all_probs, all_thresholds, all_aurocs = [], [], []\n",
    "\n",
    "for idx, (model, img_size, threshold, auroc, uses_tabular) in enumerate(model_configs):\n",
    "    print(f\"\\nInference {idx+1}/{len(model_configs)} (img_size={img_size}, tabular={uses_tabular})...\")\n",
    "    transform = get_val_transforms(img_size)\n",
    "    probs = []\n",
    "    for batch_start in tqdm(range(0, len(isic_ids), BATCH_SIZE), desc=\"Batches\"):\n",
    "        batch_ids = isic_ids[batch_start : batch_start + BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        for isic_id in batch_ids:\n",
    "            img_bytes = hdf5[isic_id][()]\n",
    "            img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = transform(image=img)[\"image\"]\n",
    "            batch_images.append(img)\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        if uses_tabular:\n",
    "            batch_tab = torch.tensor(\n",
    "                test_tabular[batch_start:batch_start+len(batch_ids)], dtype=torch.float32\n",
    "            ).to(device)\n",
    "        else:\n",
    "            batch_tab = None\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_tensor, batch_tab).squeeze(1)\n",
    "            batch_probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        probs.extend(batch_probs.tolist())\n",
    "    all_probs.append(np.array(probs))\n",
    "    all_thresholds.append(threshold)\n",
    "    all_aurocs.append(auroc)\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "hdf5.close()\n",
    "\n",
    "# ---- ENSEMBLE ----\n",
    "all_probs = np.array(all_probs)\n",
    "if ENSEMBLE_STRATEGY == \"hard_weighted\":\n",
    "    weights = np.array(all_aurocs)\n",
    "    weights = weights / weights.sum() if weights.sum() > 0 else np.ones_like(weights) / len(weights)\n",
    "    hard = np.array([(p >= t).astype(float) for p, t in zip(all_probs, all_thresholds)])\n",
    "    final_probs = np.average(hard, axis=0, weights=weights)\n",
    "    final_threshold = 0.5\n",
    "else:\n",
    "    final_probs = np.mean(all_probs, axis=0)\n",
    "    final_threshold = np.mean(all_thresholds)\n",
    "\n",
    "final_preds = (final_probs >= final_threshold).astype(int)\n",
    "\n",
    "# ---- GENERATE SUBMISSION ----\n",
    "submission = pd.DataFrame({\"isic_id\": isic_ids, \"target\": final_preds})\n",
    "submission.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUBMISSION: {OUTPUT_CSV}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Samples: {len(submission)}\")\n",
    "print(f\"  Malignant: {final_preds.sum()} ({final_preds.mean()*100:.2f}%)\")\n",
    "print(f\"  Benign: {len(final_preds) - final_preds.sum()}\")\n",
    "print(f\"  Strategy: {ENSEMBLE_STRATEGY}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5faed-65f8-43f3-b79f-f9f145b8f534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
