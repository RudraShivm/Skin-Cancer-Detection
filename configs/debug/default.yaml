# @package _global_

# default debugging setup, runs 1 full epoch
# other debugging configs can inherit from this one

# overwrite task name so debugging logs are stored in separate folder
task_name: "debug"

# disable callbacks and loggers during debugging
callbacks: null
logger: null

extras:
  ignore_warnings: False
  enforce_tags: False

# sets level of all command line loggers to 'DEBUG'
# https://hydra.cc/docs/tutorials/basic/running_your_app/logging/
hydra:
  job_logging:
    root:
      level: DEBUG

  # use this to also set hydra loggers to 'DEBUG'
  # verbose: True

trainer:
  max_epochs: 1
  accelerator: gpu
  devices: 1 # debuggers don't like multiprocessing
  # NOTE: detect_anomaly is incompatible with mixed precision (AMP) training.
  # With AMP, occasional NaN gradients are expected and handled by the GradScaler
  # (it skips the optimizer step and reduces the scale). detect_anomaly causes
  # a RuntimeError on any NaN, breaking this mechanism entirely.
  # detect_anomaly: true
  precision: 16-mixed

data:
  persistent_workers: False
  num_workers: 0 # debuggers don't like multiprocessing
  pin_memory: False # disable gpu memory pin
  data_fraction: 0.01